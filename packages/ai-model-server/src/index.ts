#!/usr/bin/env node
/**
 * AI Model MCP Server v2
 * =======================
 * 
 * å°†å¤–éƒ¨ AI æ¨¡å‹è°ƒç”¨èƒ½åŠ›æš´éœ²ä¸º MCP å·¥å…·ï¼Œè®© Cline å¯ä»¥ç›´æ¥è°ƒç”¨å¤–éƒ¨å¤§æ¨¡å‹ã€‚
 * 
 * v2 æ–°ç‰¹æ€§:
 *   - ç›´é€šæ¨¡å¼: ç›´æ¥ä¼  yunwu æ¨¡å‹åå³å¯è°ƒç”¨ï¼Œæ— éœ€é¢„å…ˆæ³¨å†Œ
 *   - åŠ¨æ€å‘ç°: è°ƒç”¨ /v1/models API è·å– yunwu æœ€æ–°å¯ç”¨æ¨¡å‹åˆ—è¡¨
 *   - é¢„è®¾æ¨¡å‹: å¸¸ç”¨æ¨¡å‹å¸¦è§’è‰²æ ‡ç­¾ï¼Œæ–¹ä¾¿æ™ºèƒ½é€‰æ‹©
 * 
 * å·¥å…·åˆ—è¡¨:
 *   - call_model      : è°ƒç”¨å¤–éƒ¨ AI æ¨¡å‹ï¼ˆæ”¯æŒé¢„è®¾IDæˆ–ç›´æ¥ä¼ æ¨¡å‹åï¼‰
 *   - list_models     : åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆé¢„è®¾ + yunwu å®æ—¶ï¼‰
 */

import { Server } from '@modelcontextprotocol/sdk/server/index.js';
import { StdioServerTransport } from '@modelcontextprotocol/sdk/server/stdio.js';
import {
  CallToolRequestSchema,
  ErrorCode,
  ListToolsRequestSchema,
  McpError,
} from '@modelcontextprotocol/sdk/types.js';
import OpenAI from 'openai';

// ============================================================
// æ¨¡å‹é…ç½®
// ============================================================

interface ModelConfig {
  id: string;
  name: string;
  role: string;
  model: string;
  maxTokens: number;
  description: string;
}

interface ProviderConfig {
  baseUrl: string;
  apiKey: string;
}

function getPresetModels(): ModelConfig[] {
  return [
    {
      id: 'claude-opus',
      name: 'Claude Opus 4.6',
      role: 'general',
      model: process.env.YUNWU_MODEL || 'claude-opus-4-6',
      maxTokens: 16000,
      description: 'æ——èˆ°çº§é€šç”¨ AIï¼Œæ“…é•¿ä¸“ä¸šä»»åŠ¡ã€é•¿ä¸Šä¸‹æ–‡ç†è§£',
    },
    {
      id: 'claude-thinking',
      name: 'Claude Opus 4.6 Thinking',
      role: 'thinking',
      model: process.env.YUNWU_THINKING_MODEL || 'claude-opus-4-6-thinking',
      maxTokens: 16000,
      description: 'æ·±åº¦åˆ†ææ¨¡å‹ï¼Œä¸“ä¸ºå¤æ‚å†³ç­–å’Œé€»è¾‘æ¨ç†è€Œç”Ÿ',
    },
    {
      id: 'search-deepseek',
      name: 'DeepSeek R1 Searching',
      role: 'search',
      model: process.env.YUNWU_SEARCH_MODEL || 'deepseek-r1-searching',
      maxTokens: 8192,
      description: 'æ·±åº¦æ¨ç†+ä¸­æ–‡ç½‘æœç´¢ï¼Œæ“…é•¿ç™¾åº¦/çŸ¥ä¹/å¾®ä¿¡å…¬ä¼—å·',
    },
    {
      id: 'search-perplexity',
      name: 'Perplexity Sonar Pro',
      role: 'search',
      model: process.env.YUNWU_SEARCH_B_MODEL || 'perplexity-sonar-pro',
      maxTokens: 8192,
      description: 'å¤šæºèšåˆæœç´¢å¼•æ“ï¼Œè‡ªåŠ¨å¼•ç”¨æ¥æº',
    },
    {
      id: 'search-gemini',
      name: 'Gemini 2.5 Flash (Search)',
      role: 'search',
      model: process.env.YUNWU_SEARCH_C_MODEL || 'gemini-2.5-flash-search',
      maxTokens: 8192,
      description: 'Googleç”Ÿæ€æœç´¢ï¼Œæ“…é•¿è‹±æ–‡æƒå¨æº',
    },
    {
      id: 'search-kimi',
      name: 'Kimi K2 (Moonshot)',
      role: 'search',
      model: process.env.YUNWU_SEARCH_D_MODEL || 'kimi-k2',
      maxTokens: 8192,
      description: 'ä¸­æ–‡æ·±åº¦æœç´¢+é•¿æ–‡æ¡£è§£æ',
    },
    {
      id: 'gpt52',
      name: 'GPT-5.2',
      role: 'racer',
      model: process.env.YUNWU_RACER_B_MODEL || 'gpt-5.2',
      maxTokens: 16000,
      description: 'OpenAI æœ€å¼ºæ——èˆ°ï¼Œæ“…é•¿å¤æ‚æ¨ç†ä¸ç»¼åˆåˆ†æ',
    },
    {
      id: 'gemini-racer',
      name: 'Gemini 3 Pro Preview',
      role: 'racer',
      model: process.env.YUNWU_RACER_C_MODEL || 'gemini-3-pro-preview',
      maxTokens: 16000,
      description: 'Google æœ€æ–°æ¨ç†æ¨¡å‹ï¼Œæ€è€ƒé“¾+å¤šæ¨¡æ€',
    },
    {
      id: 'gpt51',
      name: 'GPT-5.1',
      role: 'fast',
      model: process.env.YUNWU_FAST_MODEL || 'gpt-5.1',
      maxTokens: 8192,
      description: 'OpenAI é«˜æ€§ä»·æ¯”æ——èˆ°ï¼Œé€‚åˆå¿«é€Ÿè¾…åŠ©åˆ†æ',
    },
  ];
}

function getFallbackProviders(): Array<ModelConfig & { baseUrl: string; apiKey: string }> {
  const fallbacks: Array<ModelConfig & { baseUrl: string; apiKey: string }> = [];

  const geminiKey = process.env.GEMINI_API_KEY;
  if (geminiKey) {
    fallbacks.push({
      id: 'gemini',
      name: 'Google Gemini 3 Pro',
      role: 'fallback',
      model: process.env.GEMINI_MODEL || 'gemini-3-pro-preview',
      maxTokens: 4096,
      description: 'Google æ——èˆ°æ¨¡å‹ï¼Œå¤‡é€‰æ–¹æ¡ˆ',
      baseUrl: 'https://generativelanguage.googleapis.com/v1beta/openai/',
      apiKey: geminiKey,
    });
  }

  const githubToken = process.env.GITHUB_TOKEN;
  if (githubToken) {
    fallbacks.push({
      id: 'github-gpt4o',
      name: 'GitHub Models (GPT-4o)',
      role: 'fallback',
      model: process.env.GITHUB_MODEL || 'gpt-4o',
      maxTokens: 4096,
      description: 'GitHub Models å…œåº•æ–¹æ¡ˆ',
      baseUrl: 'https://models.inference.ai.azure.com',
      apiKey: githubToken,
    });
  }

  return fallbacks;
}

// ============================================================
// Thinking æ ‡ç­¾æ¸…ç†
// ============================================================

function cleanThinkingTags(text: string): { content: string; thinking: string } {
  if (!text) return { content: '', thinking: '' };

  const thinkingParts: string[] = [];
  const pattern = /<[Tt]hinking>([\s\S]*?)<\/[Tt]hinking>/g;

  let match;
  while ((match = pattern.exec(text)) !== null) {
    thinkingParts.push(match[1].trim());
  }

  let cleaned = text.replace(/<[Tt]hinking>[\s\S]*?<\/[Tt]hinking>/g, '');
  cleaned = cleaned.replace(/\n{3,}/g, '\n\n').trim();

  return {
    content: cleaned,
    thinking: thinkingParts.join('\n\n---\n\n'),
  };
}

// ============================================================
// MCP Server
// ============================================================

class AIModelServer {
  private server: Server;
  private presetModels: Map<string, ModelConfig>;
  private modelNameIndex: Map<string, ModelConfig>;
  private fallbacks: Array<ModelConfig & { baseUrl: string; apiKey: string }>;
  private yunwuConfig: ProviderConfig | null;
  private cachedYunwuModels: string[] | null = null;
  private cacheTimestamp: number = 0;
  private readonly CACHE_TTL = 5 * 60 * 1000;

  constructor() {
    const yunwuKey = process.env.YUNWU_API_KEY;
    const yunwuBase = process.env.YUNWU_BASE_URL || 'http://hw.yunwu.ai:3000/v1';
    this.yunwuConfig = yunwuKey ? { baseUrl: yunwuBase, apiKey: yunwuKey } : null;

    this.presetModels = new Map();
    this.modelNameIndex = new Map();

    if (this.yunwuConfig) {
      for (const m of getPresetModels()) {
        this.presetModels.set(m.id, m);
        this.modelNameIndex.set(m.model, m);
      }
    }

    this.fallbacks = getFallbackProviders();
    for (const fb of this.fallbacks) {
      this.presetModels.set(fb.id, fb);
      this.modelNameIndex.set(fb.model, fb);
    }

    this.server = new Server(
      { name: 'ai-model-server', version: '2.0.0' },
      { capabilities: { tools: {} } }
    );

    this.setupToolHandlers();
    this.server.onerror = (error) => console.error('[MCP Error]', error);
    process.on('SIGINT', async () => {
      await this.server.close();
      process.exit(0);
    });
  }

  private resolveModel(modelId: string): {
    model: string; name: string; maxTokens: number; baseUrl: string; apiKey: string;
  } | null {
    const preset = this.presetModels.get(modelId);
    if (preset) {
      const fb = this.fallbacks.find(f => f.id === modelId);
      if (fb) {
        return { model: fb.model, name: fb.name, maxTokens: fb.maxTokens, baseUrl: fb.baseUrl, apiKey: fb.apiKey };
      }if (this.yunwuConfig) {
        return { model: preset.model, name: preset.name, maxTokens: preset.maxTokens, baseUrl: this.yunwuConfig.baseUrl, apiKey: this.yunwuConfig.apiKey };
      }
    }

    const byName = this.modelNameIndex.get(modelId);
    if (byName) {
      const fb = this.fallbacks.find(f => f.model === modelId);
      if (fb) {
        return { model: fb.model, name: fb.name, maxTokens: fb.maxTokens, baseUrl: fb.baseUrl, apiKey: fb.apiKey };
      }
      if (this.yunwuConfig) {
        return { model: byName.model, name: byName.name, maxTokens: byName.maxTokens, baseUrl: this.yunwuConfig.baseUrl, apiKey: this.yunwuConfig.apiKey };
      }
    }

    if (this.yunwuConfig) {
      return { model: modelId, name: `${modelId} (ç›´é€š)`, maxTokens: 16000, baseUrl: this.yunwuConfig.baseUrl, apiKey: this.yunwuConfig.apiKey };
    }

    return null;
  }

  private async fetchYunwuModels(): Promise<string[]> {
    if (!this.yunwuConfig) return [];
    if (this.cachedYunwuModels && (Date.now() - this.cacheTimestamp) < this.CACHE_TTL) {
      return this.cachedYunwuModels;
    }

    try {
      const client = new OpenAI({ baseURL: this.yunwuConfig.baseUrl, apiKey: this.yunwuConfig.apiKey, timeout: 10000 });
      const response = await client.models.list();
      const models: string[] = [];
      for await (const model of response) { models.push(model.id); }
      models.sort();
      this.cachedYunwuModels = models;
      this.cacheTimestamp = Date.now();
      return models;
    } catch (error) {
      console.error('[fetchYunwuModels]', error);
      return [];
    }
  }

  private setupToolHandlers() {
    this.server.setRequestHandler(ListToolsRequestSchema, async () => {
      const presetIds = Array.from(this.presetModels.keys());
      return {
        tools: [
          {
            name: 'call_model',
            description: `è°ƒç”¨å¤–éƒ¨ AI æ¨¡å‹ã€‚æ”¯æŒ ${this.presetModels.size} ä¸ªé¢„è®¾æ¨¡å‹ï¼š${Array.from(this.presetModels.values()).map(m => `${m.id}(${m.role})`).join('ã€')}ã€‚ä¹Ÿæ”¯æŒç›´æ¥ä¼  yunwu æ¨¡å‹åï¼ˆå¦‚ deepseek-v3ï¼‰è¿›è¡Œç›´é€šè°ƒç”¨ã€‚`,
            inputSchema: {
              type: 'object' as const,
              properties: {
                model_id: { type: 'string', description: `æ¨¡å‹ IDã€‚å¯é€‰å€¼: ${presetIds.join(', ')}ã€‚ä¹Ÿå¯ç›´æ¥ä¼  yunwu æ¨¡å‹å…¨åã€‚é»˜è®¤: claude-opus` },
                prompt: { type: 'string', description: 'ç”¨æˆ·æç¤ºè¯ï¼ˆå¿…å¡«ï¼‰' },
                system_prompt: { type: 'string', description: 'ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰' },
                max_tokens: { type: 'number', description: 'æœ€å¤§è¾“å‡º token æ•°ï¼ˆé»˜è®¤: 16000ï¼‰' },
                clean_thinking: { type: 'boolean', description: 'æ˜¯å¦æ¸…ç† <thinking> æ ‡ç­¾ï¼ˆé»˜è®¤: trueï¼‰' },
              },
              required: ['prompt'],
            },
          },
          {
            name: 'list_models',
            description: 'åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å¤–éƒ¨ AI æ¨¡å‹åŠå…¶è§’è‰²ã€èƒ½åŠ›æè¿°',
            inputSchema: { type: 'object' as const, properties: {} },
          },
        ],};
    });

    this.server.setRequestHandler(CallToolRequestSchema, async (request) => {
      switch (request.params.name) {
        case 'call_model': return this.handleCallModel(request.params.arguments);
        case 'list_models': return this.handleListModels();
        default: throw new McpError(ErrorCode.MethodNotFound, `Unknown tool: ${request.params.name}`);
      }
    });
  }

  private async handleCallModel(args: Record<string, unknown> | undefined) {
    if (!args || typeof args.prompt !== 'string' || !args.prompt) {
      throw new McpError(ErrorCode.InvalidParams, 'ç¼ºå°‘å¿…å¡«å‚æ•°: prompt');
    }

    const modelId = (args.model_id as string) || 'claude-opus';
    const prompt = args.prompt as string;
    const systemPrompt = (args.system_prompt as string) || '';
    const cleanThinking = args.clean_thinking !== false;

    const resolved = this.resolveModel(modelId);
    if (!resolved) {
      return { content: [{ type: 'text', text: `âŒ æ— æ³•è§£ææ¨¡å‹: ${modelId}\n\næ²¡æœ‰å¯ç”¨çš„ API é…ç½®ã€‚` }], isError: true };
    }

    const maxTokens = (args.max_tokens as number) || resolved.maxTokens;
    const startTime = Date.now();

    try {
      const client = new OpenAI({ baseURL: resolved.baseUrl, apiKey: resolved.apiKey, timeout: 300000 });
      const messages: Array<{ role: 'system' | 'user'; content: string }> = [];
      if (systemPrompt) messages.push({ role: 'system', content: systemPrompt });
      messages.push({ role: 'user', content: prompt });

      const response = await client.chat.completions.create({ model: resolved.model, max_tokens: maxTokens, messages });
      const rawContent = response.choices[0]?.message?.content || '';
      const duration = ((Date.now() - startTime) / 1000).toFixed(1);

      let finalContent: string;
      let thinkingContent = '';
      if (cleanThinking) {
        const cleaned = cleanThinkingTags(rawContent);
        finalContent = cleaned.content;
        thinkingContent = cleaned.thinking;
      } else {
        finalContent = rawContent;
      }

      const meta = [
        `ğŸ“¡ æ¨¡å‹: ${resolved.name} (${resolved.model})`,
        `â±ï¸ è€—æ—¶: ${duration}ç§’`,
        `ğŸ“Š å†…å®¹: ${finalContent.length} å­—ç¬¦`,
      ];
      if (thinkingContent) meta.push(`ğŸ§  æ€è€ƒè¿‡ç¨‹: ${thinkingContent.length} å­—ç¬¦`);

      const parts: Array<{ type: string; text: string }> = [];
      parts.push({ type: 'text', text: finalContent });
      parts.push({ type: 'text', text: `\n---\n${meta.join(' | ')}` });
      if (thinkingContent) {
        parts.push({ type: 'text', text: `\n<details><summary>ğŸ§  æ€è€ƒè¿‡ç¨‹</summary>\n\n${thinkingContent}\n\n</details>` });
      }

      return { content: parts };
    } catch (error: unknown) {
      const duration = ((Date.now() - startTime) / 1000).toFixed(1);
      const errMsg = error instanceof Error ? error.message : String(error);
      return { content: [{ type: 'text', text: `âŒ è°ƒç”¨æ¨¡å‹ ${resolved.name} å¤±è´¥ (${duration}ç§’)\n\né”™è¯¯: ${errMsg}` }], isError: true };
    }
  }

  private async handleListModels() {
    const roleIcons: Record<string, string> = {
      general: 'ğŸ’¬', thinking: 'ğŸ§ ', search: 'ğŸ”', racer: 'ğŸ‡', fast: 'âš¡', fallback: 'ğŸ”„',
    };

    const lines: string[] = [];
    lines.push(`## ğŸ“Œ é¢„è®¾æ¨¡å‹ (${this.presetModels.size} ä¸ª)\n`);
    lines.push('| çŸ­ ID | åç§° | è§’è‰² | æ¨¡å‹å | æè¿° |');
    lines.push('|---|---|---|---|---|');

    for (const m of this.presetModels.values()) {
      const icon = roleIcons[m.role] || 'ğŸ¤–';
      lines.push(`| \`${m.id}\` | ${icon} ${m.name} | ${m.role} | \`${m.model}\` | ${m.description} |`);
    }

    if (this.yunwuConfig) {
      lines.push('\n## ğŸŒ Yunwu å¯ç”¨æ¨¡å‹ï¼ˆå®æ—¶æŸ¥è¯¢ï¼‰\n');
      try {
        const yunwuModels = await this.fetchYunwuModels();
        if (yunwuModels.length > 0) {
          const presetModelNames = new Set(
            Array.from(this.presetModels.values())
              .filter(m => !this.fallbacks.find(f => f.id === m.id))
              .map(m => m.model)
          );
          const presetList: string[] = [];
          const otherList: string[] = [];
          for (const name of yunwuModels) {
            if (presetModelNames.has(name)) presetList.push(`âœ… \`${name}\``);
            else otherList.push(`\`${name}\``);
          }
          if (presetList.length > 0) lines.push(`**å·²é¢„è®¾** (${presetList.length}): ${presetList.join(', ')}\n`);
          if (otherList.length > 0) lines.push(`**å¯ç›´é€šè°ƒç”¨** (${otherList.length}): ${otherList.join(', ')}\n`);lines.push(`\n> ğŸ’¡ ç›´é€šè°ƒç”¨: \`call_model(model_id="æ¨¡å‹å", prompt="...")\``);
        } else {
          lines.push('> âš ï¸ æ— æ³•è·å– yunwu æ¨¡å‹åˆ—è¡¨ï¼Œä½†ç›´é€šè°ƒç”¨ä»ç„¶å¯ç”¨');
        }
      } catch {
        lines.push('> âš ï¸ è·å– yunwu æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼Œä½†ç›´é€šè°ƒç”¨ä»ç„¶å¯ç”¨');
      }
    }

    lines.push('\n---');
    lines.push('**ä½¿ç”¨æ–¹å¼**: `call_model(model_id="claude-opus", prompt="...")`');

    return { content: [{ type: 'text', text: lines.join('\n') }] };
  }

  async run() {
    const transport = new StdioServerTransport();
    await this.server.connect(transport);
    console.error(`AI Model MCP Server v2 running (${this.presetModels.size} preset models, pass-through enabled)`);
  }
}

const server = new AIModelServer();
server.run().catch(console.error);